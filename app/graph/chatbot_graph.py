from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph

from app.velog.vector_store import VelogVectorStore


def print_search_result(search_result):
    documents = search_result.get('documents', [[]])[0]
    metadatas = search_result.get('metadatas', [[]])[0]
    ids = search_result.get('ids', [[]])[0]

    for i, (doc, meta, id) in enumerate(zip(documents, metadatas, ids), 1):
        print(f"============ ğŸ” ê²€ìƒ‰ ê²°ê³¼ {i} ============")
        print(f"ID: {id}")
        print(f"Title: {meta.get('title', 'ì œëª©ì—†ìŒ')}")
        print(f"URL: {meta.get('url', 'URLì—†ìŒ')}")
        print(f"Content (ì•ë¶€ë¶„): {doc[:300]}...\n")  # ì• 200ìë§Œ ë¯¸ë¦¬ë³´ê¸°ë¡œ ì¶œë ¥
        print(f"========================================")


# ìƒíƒœ ì •ì˜
class GraphState(dict):
    question: str
    search_result: dict
    answer: str


# ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”
vector_store = VelogVectorStore()

# LLM ì´ˆê¸°í™” (LangGraphëŠ” LangChain LLM ì‚¬ìš©)
llm = ChatOpenAI(model="gpt-3.5-turbo-0125", temperature=0.3)


# ê²€ìƒ‰ ë…¸ë“œ
def search_node(state: GraphState):
    query = state['question']
    search_result = vector_store.search(query, top_k=3)
    print_search_result(search_result)
    state['search_result'] = search_result
    return state


# LLM ë‹µë³€ ë…¸ë“œ
def answer_node(state: GraphState):
    question = state['question']
    search_result = state['search_result']

    documents = search_result['documents'][0]
    metadatas = search_result['metadatas'][0]

    # context êµ¬ì„±
    context = ""
    for doc, meta in zip(documents, metadatas):
        title = meta.get('title', 'ì œëª© ì—†ìŒ')
        url = meta.get('url', 'URL ì—†ìŒ')
        context += f"ì œëª©: {title}\në§í¬: {url}\në³¸ë¬¸: {doc}\n\n"

    system_prompt = (
        "ë„ˆëŠ” ë‚˜ì˜ ë²¨ë¡œê·¸ ë¸”ë¡œê·¸ í¬ìŠ¤íŒ…ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì±—ë´‡ì´ë‹¤.\n"
        "ë‹¤ìŒì€ ê´€ë ¨ ë¸”ë¡œê·¸ ê¸€ì´ë‹¤:\n"
        f"{context}\n"
        "ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ìµœëŒ€í•œ ìƒì„¸íˆ ë‹µë³€í•´ì¤˜."
    )

    response = llm.invoke([
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": question}
    ])

    state['answer'] = response.content
    return state


# LangGraph ì •ì˜
graph = StateGraph(GraphState)

graph.add_node("search", search_node)
graph.add_node("generate_answer", answer_node)

# íë¦„ ì—°ê²°
graph.set_entry_point("search")
graph.add_edge("search", "generate_answer")
graph.set_finish_point("generate_answer")

# ì»´íŒŒì¼
runnable = graph.compile()
